{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb8dfc4",
   "metadata": {},
   "source": [
    "# BeautifulSoup Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b812557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\divith raj\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\divith raj\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\divith raj\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\divith raj\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\divith raj\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: bs4 in c:\\users\\divith raj\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\divith raj\\anaconda3\\lib\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\divith raj\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2454055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce4cd53",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aec5bae",
   "metadata": {},
   "source": [
    "Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d319b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_content(url):\n",
    "    page=requests.get(url)\n",
    "    \n",
    "    soup=BeautifulSoup(page.content) #downloads all the html content of the page\n",
    "    \n",
    "    header=[]\n",
    "\n",
    "    for i in soup.find_all(['h1','h2','h3','h4','h5','h6']):\n",
    "        header.append(i.text)\n",
    "\n",
    "    head=pd.DataFrame({'Headers':header})\n",
    "    return(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41d2c175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Main Page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welcome to Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From today's featured article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Did you know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>On this day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Today's featured picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Other areas of Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wikipedia's sister projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wikipedia languages</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Headers\n",
       "0                      Main Page\n",
       "1           Welcome to Wikipedia\n",
       "2  From today's featured article\n",
       "3               Did you know ...\n",
       "4                    In the news\n",
       "5                    On this day\n",
       "6       Today's featured picture\n",
       "7       Other areas of Wikipedia\n",
       "8    Wikipedia's sister projects\n",
       "9            Wikipedia languages"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_content('https://en.wikipedia.org/wiki/Main_Page')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f0f58",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020295df",
   "metadata": {},
   "source": [
    "Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "from https://presidentofindia.nic.in/former-presidents.htm and make data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11143417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "page=requests.get('https://presidentofindia.nic.in/former-presidents')\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6779c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def president_of_india(url):\n",
    "    page=requests.get(url)\n",
    "    \n",
    "    soup=BeautifulSoup(page.content) #downloads all the html content of the page\n",
    "    \n",
    "    name=[]\n",
    "    \n",
    "    term_of_year=[]\n",
    "\n",
    "    for i in soup.find_all('h3'):\n",
    "        name.append(i.text)\n",
    "    for i in soup.find_all('h5'):\n",
    "        term_of_year.append(i.text)\n",
    "\n",
    "    president=pd.DataFrame({'President Name':name,'Term Of Year':term_of_year})\n",
    "    return(president)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f136e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>President Name</th>\n",
       "      <th>Term Of Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind</td>\n",
       "      <td>14th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee</td>\n",
       "      <td>13th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil</td>\n",
       "      <td>12th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam</td>\n",
       "      <td>11th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan</td>\n",
       "      <td>10th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma</td>\n",
       "      <td>9th  President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman</td>\n",
       "      <td>8th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh</td>\n",
       "      <td>7th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy</td>\n",
       "      <td>6th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed</td>\n",
       "      <td>5th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri</td>\n",
       "      <td>4th President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain</td>\n",
       "      <td>3rd President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan</td>\n",
       "      <td>2nd President of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad</td>\n",
       "      <td>1st President of India</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  President Name             Term Of Year\n",
       "0           Shri Ram Nath Kovind  14th President of India\n",
       "1          Shri Pranab Mukherjee  13th President of India\n",
       "2   Smt Pratibha Devisingh Patil  12th President of India\n",
       "3         DR. A.P.J. Abdul Kalam  11th President of India\n",
       "4           Shri K. R. Narayanan  10th President of India\n",
       "5        Dr Shankar Dayal Sharma  9th  President of India\n",
       "6            Shri R Venkataraman   8th President of India\n",
       "7               Giani Zail Singh   7th President of India\n",
       "8      Shri Neelam Sanjiva Reddy   6th President of India\n",
       "9       Dr. Fakhruddin Ali Ahmed   5th President of India\n",
       "10  Shri Varahagiri Venkata Giri   4th President of India\n",
       "11              Dr. Zakir Husain   3rd President of India\n",
       "12  Dr. Sarvepalli Radhakrishnan   2nd President of India\n",
       "13           Dr. Rajendra Prasad   1st President of India"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "president_of_india('https://presidentofindia.nic.in/former-presidents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47300161",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a857314d",
   "metadata": {},
   "source": [
    " Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\u0002\n",
    " a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    " b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    " c) Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92a303",
   "metadata": {},
   "source": [
    "Ans a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "751b4efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mens_team_rankings(url):\n",
    "    page=requests.get(url)\n",
    "    \n",
    "    soup=BeautifulSoup(page.content) #downloads all the html content of the page\n",
    "    \n",
    "    team=[]\n",
    "    \n",
    "    matches=[]\n",
    "    \n",
    "    pts=[]\n",
    "    \n",
    "    rating=[]\n",
    "    for i in soup.find_all('span',class_=\"si-fname si-text\"):\n",
    "        team.append(i.text)\n",
    "    for i in soup.find_all('div',class_=\"si-text\"):\n",
    "        result=re.findall('[\\d]{2}',string)\n",
    "        matches.append(result)\n",
    "    for i in soup.find_all('div',class_=\"si-table-data si-pts\"):\n",
    "        pts.append(i.text)\n",
    "    for i in soup.find_all('div',class_=\"si-table-data si-rating\"):\n",
    "        rating.append(i.text)\n",
    "\n",
    "    ranking_team=pd.DataFrame({'Team':team,'Matches':matches,'Points':pts,'Rating':rating})\n",
    "    \n",
    "    return(ranking_team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_team_rankings(\"https://www.icc-cricket.com/rankings/team-rankings/mens/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f672fa9",
   "metadata": {},
   "source": [
    "b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea17a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mens_odi_batsmen(url):\n",
    "    page=requests.get(url)\n",
    "    \n",
    "    soup=BeautifulSoup(page.content) #downloads all the html content of the page\n",
    "    \n",
    "    player=[]\n",
    "    \n",
    "    matches=[]\n",
    "    \n",
    "    rating=[]\n",
    "    for i in soup.find_all('span',class_=\"si-fname si-text\"):\n",
    "        player.append(i.text)\n",
    "    for i in soup.find_all('div',class_=\"si-text\"):\n",
    "        result=re.findall('[\\d]{2}',string)\n",
    "        matches.append(result)\n",
    "    for i in soup.find_all('div',class_=\"si-table-data si-rating\"):\n",
    "        rating.append(i.text)\n",
    "\n",
    "    ranking_bat=pd.DataFrame({'Player':player,'Matches':matches,'Rating':rating})\n",
    "    \n",
    "    return(ranking_bat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0775b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_odi_batsmen(\"https://www.icc-cricket.com/rankings/team-rankings/mens/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d246e4",
   "metadata": {},
   "source": [
    "c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mens_odi_bowler(url):\n",
    "    page=requests.get(url)\n",
    "    \n",
    "    soup=BeautifulSoup(page.content) #downloads all the html content of the page\n",
    "    \n",
    "    team=[]\n",
    "    \n",
    "    matches=[]\n",
    "    \n",
    "    pts=[]\n",
    "    \n",
    "    rating=[]\n",
    "    for i in soup.find_all('span',class_=\"si-fname si-text\"):\n",
    "        player.append(i.text)\n",
    "    for i in soup.find_all('div',class_=\"si-text\"):\n",
    "        result=re.findall('[\\d]{2}',string)\n",
    "        matches.append(result)\n",
    "    for i in soup.find_all('div',class_=\"si-table-data si-rating\"):\n",
    "        rating.append(i.text)\n",
    "\n",
    "    ranking_bowl=pd.DataFrame({'Player':player,'Matches':matches,'Rating':rating})\n",
    "    \n",
    "    return(ranking_bowl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_odi_bowler(\"https://www.icc-cricket.com/rankings/team-rankings/mens/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d2747",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b4462dd",
   "metadata": {},
   "source": [
    "Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\u0002\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a76e23",
   "metadata": {},
   "source": [
    "a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8422f5ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m     ranking_team\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam\u001b[39m\u001b[38;5;124m'\u001b[39m:team,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatches\u001b[39m\u001b[38;5;124m'\u001b[39m:matches,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPoints\u001b[39m\u001b[38;5;124m'\u001b[39m:pts,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRating\u001b[39m\u001b[38;5;124m'\u001b[39m:rating})\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(ranking_team)\n\u001b[1;32m---> 27\u001b[0m womens_team_rankings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.icc-cricket.com/rankings/team-rankings/mens/odi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m, in \u001b[0;36mwomens_team_rankings\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwomens_team_rankings\u001b[39m(url):\n\u001b[1;32m----> 2\u001b[0m     page\u001b[38;5;241m=\u001b[39mrequests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m      4\u001b[0m     soup\u001b[38;5;241m=\u001b[39mBeautifulSoup(page\u001b[38;5;241m.\u001b[39mcontent) \u001b[38;5;66;03m#downloads all the html content of the page\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     team\u001b[38;5;241m=\u001b[39m[]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "def womens_team_rankings(url):\n",
    "    page=requests.get(url)\n",
    "    \n",
    "    soup=BeautifulSoup(page.content) #downloads all the html content of the page\n",
    "    \n",
    "    team=[]\n",
    "    \n",
    "    matches=[]\n",
    "    \n",
    "    pts=[]\n",
    "    \n",
    "    rating=[]\n",
    "    for i in soup.find_all('span',class_=\"si-fname si-text\"):\n",
    "        team.append(i.text)\n",
    "    for i in soup.find_all('div',class_=\"si-text\"):\n",
    "        result=re.findall('[\\d]{2}',string)\n",
    "        matches.append(result)\n",
    "    for i in soup.find_all('div',class_=\"si-table-data si-pts\"):\n",
    "        pts.append(i.text)\n",
    "    for i in soup.find_all('div',class_=\"si-table-data si-rating\"):\n",
    "        rating.append(i.text)\n",
    "\n",
    "    ranking_team=pd.DataFrame({'Team':team,'Matches':matches,'Points':pts,'Rating':rating})\n",
    "    \n",
    "    return(ranking_team)\n",
    "\n",
    "womens_team_rankings(\"https://www.icc-cricket.com/rankings/team-rankings/mens/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19986292",
   "metadata": {},
   "source": [
    "b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af473116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def womens_odi_batsmen(url):\n",
    "    page=requests.get(url)\n",
    "    \n",
    "    soup=BeautifulSoup(page.content) #downloads all the html content of the page\n",
    "    \n",
    "    player=[]\n",
    "    \n",
    "    matches=[]\n",
    "    \n",
    "    rating=[]\n",
    "    for i in soup.find_all('span',class_=\"si-fname si-text\"):\n",
    "        player.append(i.text)\n",
    "    for i in soup.find_all('div',class_=\"si-text\"):\n",
    "        result=re.findall('[\\d]{2}',string)\n",
    "        matches.append(result)\n",
    "    for i in soup.find_all('div',class_=\"si-table-data si-rating\"):\n",
    "        rating.append(i.text)\n",
    "\n",
    "    ranking_bat=pd.DataFrame({'Player':player,'Matches':matches,'Rating':rating})\n",
    "    \n",
    "    return(ranking_bat)\n",
    "\n",
    "womens_odi_batsmen(\"https://www.icc-cricket.com/rankings/team-rankings/mens/odi\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa734f56",
   "metadata": {},
   "source": [
    "c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def womens_odi_bowler(url):\n",
    "    page=requests.get(url)\n",
    "    \n",
    "    soup=BeautifulSoup(page.content) #downloads all the html content of the page\n",
    "    \n",
    "    team=[]\n",
    "    \n",
    "    matches=[]\n",
    "    \n",
    "    rating=[]\n",
    "    for i in soup.find_all('span',class_=\"si-fname si-text\"):\n",
    "        player.append(i.text)\n",
    "    for i in soup.find_all('div',class_=\"si-text\"):\n",
    "        result=re.findall('[\\d]{2}',string)\n",
    "        matches.append(result)\n",
    "    for i in soup.find_all('div',class_=\"si-table-data si-rating\"):\n",
    "        rating.append(i.text)\n",
    "\n",
    "    ranking_bowl=pd.DataFrame({'Player':player,'Matches':matches,'Rating':rating})\n",
    "    \n",
    "    return(ranking_bowl)\n",
    "\n",
    "\n",
    "womens_odi_bowler(\"https://www.icc-cricket.com/rankings/team-rankings/mens/odi\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a02b0",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "raw",
   "id": "256bbf13",
   "metadata": {},
   "source": [
    "Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "make data frame\u0002\n",
    "i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7884cf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 26\u001b[0m\n\u001b[0;32m     21\u001b[0m     latest_news\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeadline\u001b[39m\u001b[38;5;124m'\u001b[39m:headline,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m:time,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m:url})\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(latest_news)\n\u001b[1;32m---> 26\u001b[0m news(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.cnbc.com/world/?region=world\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 21\u001b[0m, in \u001b[0;36mnews\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m,href\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     18\u001b[0m     url\u001b[38;5;241m.\u001b[39mappend(i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 21\u001b[0m latest_news\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeadline\u001b[39m\u001b[38;5;124m'\u001b[39m:headline,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m:time,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m:url})\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(latest_news)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    703\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    704\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    705\u001b[0m     )\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 709\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    479\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:655\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    653\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    659\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    660\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "def news(url):\n",
    "    page=requests.get(url)\n",
    "    \n",
    "    soup=BeautifulSoup(page.content) #downloads all the html content of the page\n",
    "    \n",
    "    headline=[]\n",
    "    \n",
    "    time=[]\n",
    "    \n",
    "    url=[]\n",
    "    \n",
    "    for i in soup.find_all('a',class_=\"LatestNews-headline\"):\n",
    "        headline.append(i.text)\n",
    "\n",
    "    for i in soup.find_all('span',class_=\"LatestNews-wrapper\"):\n",
    "        time.append(i.text)\n",
    "    for i in soup.find_all('a',href=True):\n",
    "        url.append(i['href'])\n",
    "    \n",
    "\n",
    "    latest_news=pd.DataFrame({'Headline':headline,'Time':time,'URL':url})\n",
    "    \n",
    "    return(latest_news)\n",
    "\n",
    "\n",
    "news('https://www.cnbc.com/world/?region=world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6962e45",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0791dbc6",
   "metadata": {},
   "source": [
    "Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details and make data frame\u0002i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15ad70b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>David Silver, Satinder Singh, Doina Precup, Ri...</td>\n",
       "      <td>October 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Tim Miller</td>\n",
       "      <td>February 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Margaret A. Boden</td>\n",
       "      <td>August 1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Guni Sharon, Roni Stern, Ariel Felner, Nathan ...</td>\n",
       "      <td>February 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Knowledge graphs as tools for explainable mach...</td>\n",
       "      <td>Ilaria Tiddi, Stefan Schlobach</td>\n",
       "      <td>January 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Henry Prakken, Giovanni Sartor</td>\n",
       "      <td>October 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Richard S. Sutton, Doina Precup, Satinder Singh</td>\n",
       "      <td>August 1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Kjersti Aas, Martin Jullum, Anders Løland</td>\n",
       "      <td>September 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Wenhan Luo, Junliang Xing and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Saurabh Arora, Prashant Doshi</td>\n",
       "      <td>August 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>Jasper van der Waa, Elisabeth Nieuwburg, Anita...</td>\n",
       "      <td>February 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Explainable AI tools for legal reasoning about...</td>\n",
       "      <td>Joe Collenette, Katie Atkinson, Trevor Bench-C...</td>\n",
       "      <td>April 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hard choices in artificial intelligence</td>\n",
       "      <td>Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz</td>\n",
       "      <td>November 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Assessing the communication gap between AI mod...</td>\n",
       "      <td>Oskar Wysocki, Jessica Katharine Davies and 5 ...</td>\n",
       "      <td>March 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...</td>\n",
       "      <td>May 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Nolan Bard, Jakob N. Foerster and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Ron Kohavi, George H. John</td>\n",
       "      <td>December 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Séverin Lemaignan, Mathieu Warnier and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz</td>\n",
       "      <td>June 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The multifaceted impact of Ada Lovelace in the...</td>\n",
       "      <td>Luigia Carlucci Aiello</td>\n",
       "      <td>June 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Robot ethics: Mapping the issues for a mechani...</td>\n",
       "      <td>Patrick Lin, Keith Abney, George Bekey</td>\n",
       "      <td>April 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Reward (Mis)design for autonomous driving</td>\n",
       "      <td>W. Bradley Knox, Alessandro Allievi and 3 more</td>\n",
       "      <td>March 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Planning and acting in partially observable st...</td>\n",
       "      <td>Leslie Pack Kaelbling, Michael L. Littman, Ant...</td>\n",
       "      <td>May 1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What do we want from Explainable Artificial In...</td>\n",
       "      <td>Markus Langer, Daniel Oster and 6 more</td>\n",
       "      <td>July 2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paper Title  \\\n",
       "0                                    Reward is enough   \n",
       "1   Explanation in artificial intelligence: Insigh...   \n",
       "2              Creativity and artificial intelligence   \n",
       "3   Conflict-based search for optimal multi-agent ...   \n",
       "4   Knowledge graphs as tools for explainable mach...   \n",
       "5   Law and logic: A review from an argumentation ...   \n",
       "6   Between MDPs and semi-MDPs: A framework for te...   \n",
       "7   Explaining individual predictions when feature...   \n",
       "8       Multiple object tracking: A literature review   \n",
       "9   A survey of inverse reinforcement learning: Ch...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11  Explainable AI tools for legal reasoning about...   \n",
       "12            Hard choices in artificial intelligence   \n",
       "13  Assessing the communication gap between AI mod...   \n",
       "14  Explaining black-box classifiers using post-ho...   \n",
       "15  The Hanabi challenge: A new frontier for AI re...   \n",
       "16              Wrappers for feature subset selection   \n",
       "17  Artificial cognition for social human–robot in...   \n",
       "18  A review of possible effects of cognitive bias...   \n",
       "19  The multifaceted impact of Ada Lovelace in the...   \n",
       "20  Robot ethics: Mapping the issues for a mechani...   \n",
       "21          Reward (Mis)design for autonomous driving   \n",
       "22  Planning and acting in partially observable st...   \n",
       "23  What do we want from Explainable Artificial In...   \n",
       "\n",
       "                                              Authors  Published Date  \n",
       "0   David Silver, Satinder Singh, Doina Precup, Ri...    October 2021  \n",
       "1                                         Tim Miller    February 2019  \n",
       "2                                  Margaret A. Boden      August 1998  \n",
       "3   Guni Sharon, Roni Stern, Ariel Felner, Nathan ...   February 2015  \n",
       "4                     Ilaria Tiddi, Stefan Schlobach     January 2022  \n",
       "5                     Henry Prakken, Giovanni Sartor     October 2015  \n",
       "6    Richard S. Sutton, Doina Precup, Satinder Singh      August 1999  \n",
       "7          Kjersti Aas, Martin Jullum, Anders Løland   September 2021  \n",
       "8                Wenhan Luo, Junliang Xing and 4 more      April 2021  \n",
       "9                      Saurabh Arora, Prashant Doshi      August 2021  \n",
       "10  Jasper van der Waa, Elisabeth Nieuwburg, Anita...   February 2021  \n",
       "11  Joe Collenette, Katie Atkinson, Trevor Bench-C...      April 2023  \n",
       "12  Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz    November 2021  \n",
       "13  Oskar Wysocki, Jessica Katharine Davies and 5 ...      March 2023  \n",
       "14  Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...        May 2021  \n",
       "15          Nolan Bard, Jakob N. Foerster and 13 more      March 2020  \n",
       "16                        Ron Kohavi, George H. John    December 1997  \n",
       "17      Séverin Lemaignan, Mathieu Warnier and 3 more       June 2017  \n",
       "18   Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz        June 2021  \n",
       "19                            Luigia Carlucci Aiello        June 2016  \n",
       "20            Patrick Lin, Keith Abney, George Bekey       April 2011  \n",
       "21     W. Bradley Knox, Alessandro Allievi and 3 more      March 2023  \n",
       "22  Leslie Pack Kaelbling, Michael L. Littman, Ant...        May 1998  \n",
       "23             Markus Langer, Daniel Oster and 6 more       July 2021  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def AI(url):\n",
    "    page=requests.get(url)\n",
    "    \n",
    "    soup=BeautifulSoup(page.content) #downloads all the html content of the page\n",
    "    \n",
    "    paper_title=[]\n",
    "    \n",
    "    authors=[]\n",
    "    \n",
    "    published_date=[]\n",
    "    \n",
    "    for i in soup.find_all('h2',class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\"):\n",
    "        paper_title.append(i.text)\n",
    "\n",
    "    for i in soup.find_all('span',class_=\"sc-1w3fpd7-0 dnCnAO\"):\n",
    "        authors.append(i.text)\n",
    "        \n",
    "    for i in soup.find_all('span',class_='sc-1thf9ly-2 dvggWt'):\n",
    "        published_date.append(i.text)\n",
    "\n",
    "    Articles=pd.DataFrame({'Paper Title':paper_title,'Authors':authors,'Published Date':published_date})\n",
    "    \n",
    "    return(Articles)\n",
    "\n",
    "\n",
    "AI('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b69ab6c",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ccd50",
   "metadata": {},
   "outputs": [],
   "source": [
    " Write a python program to scrape mentioned details from dineout.co.inand make data frame\u0002\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc4695ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant name</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Image URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>₹ 2,000 for 2 (approx) | Chinese, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>₹ 3,000 for 2 (approx) | Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>₹ 2,400 for 2 (approx) | North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>₹ 1,700 for 2 (approx) | North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>₹ 1,800 for 2 (approx) | North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>₹ 1,900 for 2 (approx) | North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Barbeque Times</td>\n",
       "      <td>₹ 1,500 for 2 (approx) | North Indian, Contine...</td>\n",
       "      <td>M2K Corporate Park,Sector 51, Gurgaon</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Restaurant name  \\\n",
       "0                   Castle Barbeque   \n",
       "1                        Cafe Knosh   \n",
       "2                       India Grill   \n",
       "3              The Barbeque Company   \n",
       "4                    Delhi Barbeque   \n",
       "5  The Monarch - Bar Be Que Village   \n",
       "6                The Barbeque Times   \n",
       "\n",
       "                                             Cuisine  \\\n",
       "0     ₹ 2,000 for 2 (approx) | Chinese, North Indian   \n",
       "1      ₹ 3,000 for 2 (approx) | Italian, Continental   \n",
       "2     ₹ 2,400 for 2 (approx) | North Indian, Italian   \n",
       "3     ₹ 1,700 for 2 (approx) | North Indian, Chinese   \n",
       "4              ₹ 1,800 for 2 (approx) | North Indian   \n",
       "5              ₹ 1,900 for 2 (approx) | North Indian   \n",
       "6  ₹ 1,500 for 2 (approx) | North Indian, Contine...   \n",
       "\n",
       "                                            Location Ratings  \\\n",
       "0                     Connaught Place, Central Delhi       4   \n",
       "1  The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
       "2               Hilton Garden Inn,Saket, South Delhi     3.9   \n",
       "3                 Gardens Galleria,Sector 38A, Noida     3.9   \n",
       "4     Taurus Sarovar Portico,Mahipalpur, South Delhi     3.7   \n",
       "5  Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
       "6              M2K Corporate Park,Sector 51, Gurgaon     4.1   \n",
       "\n",
       "                                           Image URL  \n",
       "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def restaurant(url):\n",
    "    page=requests.get(url)\n",
    "    \n",
    "    soup=BeautifulSoup(page.content) #downloads all the html content of the page\n",
    "    \n",
    "    rest_name=[]\n",
    "    \n",
    "    cuisine=[]\n",
    "    \n",
    "    loc=[]\n",
    "    \n",
    "    rating=[]\n",
    "    \n",
    "    image=[]\n",
    "    \n",
    "    \n",
    "    for i in soup.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "        rest_name.append(i.text)\n",
    "\n",
    "    for i in soup.find_all('span',class_=\"double-line-ellipsis\"):\n",
    "        cuisine.append(i.text)\n",
    "        \n",
    "    for i in soup.find_all('div',class_='restnt-loc ellipsis'):\n",
    "        loc.append(i.text)\n",
    "        \n",
    "    for i in soup.find_all('div',class_='restnt-rating rating-4'):\n",
    "        rating.append(i.text)\n",
    "        \n",
    "    for i in soup.find_all('img',class_=\"no-img\"):\n",
    "        image.append(i['data-src'])\n",
    "\n",
    "    rest=pd.DataFrame({'Restaurant name':rest_name,'Cuisine':cuisine,'Location':loc,'Ratings':rating,'Image URL':image})\n",
    "    \n",
    "    return(rest)\n",
    "\n",
    "\n",
    "restaurant('https://www.dineout.co.in/delhi-restaurants/buffet-special')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060fc775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
